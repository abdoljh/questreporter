<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Artificial Intelligence Trends  - Research Report</title>
    <style>
        @page { margin: 1in; }
        body {
            font-family: 'Times New Roman', serif;
            font-size: 12pt;
            line-height: 1.6;
            color: #000;
            max-width: 8.5in;
            margin: 0 auto;
            padding: 0.5in;
        }
        .cover {
            text-align: center;
            padding-top: 2in;
            page-break-after: always;
        }
        .cover h1 {
            font-size: 24pt;
            font-weight: bold;
            margin: 1in 0 0.5in 0;
        }
        .cover .meta {
            font-size: 14pt;
            margin: 0.25in 0;
        }
        h1 {
            font-size: 18pt;
            margin-top: 0.5in;
            border-bottom: 2px solid #333;
            padding-bottom: 0.1in;
        }
        h2 {
            font-size: 14pt;
            margin-top: 0.3in;
            font-weight: bold;
        }
        p {
            text-align: justify;
            margin: 0.15in 0;
        }
        .abstract {
            font-style: italic;
            margin: 0.25in 0.5in;
        }
        .references {
            page-break-before: always;
        }
        .ref-item {
            margin: 0.15in 0 0.15in 0.5in;
            text-indent: -0.5in;
            padding-left: 0.5in;
            font-size: 10pt;
            line-height: 1.4;
        }
        .ref-item a {
            color: #0066CC;
            text-decoration: none;
        }
        .ref-item a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="cover">
        <h1>Artificial Intelligence Trends </h1>
        <div class="meta">Research Report</div>
        <div class="meta">Subject: Computer Science </div>
        <div class="meta" style="margin-top: 1in;">
            Abdulrazzaq Al-Rashed<br>
            NEWay <br>
            February 03, 2026
        </div>
        <div class="meta" style="margin-top: 0.5in; font-size: 10pt;">
            IEEE Citation Format
        </div>
    </div>

    <h1>Executive Summary</h1>
    <p>This comprehensive report examines Artificial Intelligence Trends , analyzing key developments, challenges, and future directions based on 31 authoritative academic sources.</p>

    <h1>Abstract</h1>
    <div class="abstract">This report examines the rapidly evolving field of Artificial Intelligence Trends, focusing on five critical areas shaping the future of computer science. Through analysis of recent academic literature from 2023-2025, we investigate developments in large language models and generative AI applications, explainable AI and interpretability methods, edge computing and AI hardware acceleration, AI ethics and responsible frameworks, and multimodal AI architectures. The field of Artificial Intelligence Trends demonstrates significant advancement in transformer architectures, with models reaching up to 137B parameters and training on 1.56T words of data [Source 9]. Mathematical foundations reveal transformers as discretizations of structured integro-differential equations, where self-attention mechanisms emerge as non-local integral operators [Source 3]. Physical models now realize transformer architectures as open quantum systems in Fock space [Source 4]. This domain faces critical challenges including quadratic computational complexities limiting input sequence lengths [Source 8], efficiency optimization requirements [Source 5], and the need for comprehensive understanding of long-context processing capabilities [Source 6]. The convergence of these technological advances positions this research area at the forefront of computational innovation, demanding interdisciplinary approaches to address scalability, interpretability, and ethical deployment challenges.</div>

    <h1>Introduction</h1>
    <p>The field of Artificial Intelligence Trends represents one of the most transformative forces in modern computer science, fundamentally reshaping how we approach computational problems and human-machine interaction. This research area has witnessed unprecedented growth, particularly in the development of large language models and transformer architectures that have revolutionized natural language processing capabilities. Recent advances demonstrate the mathematical sophistication underlying these systems, where researchers have established that transformers can be understood as discretizations of structured integro-differential equations [Source 1]. The self-attention mechanism, central to transformer success, emerges naturally as a non-local integral operator, providing theoretical grounding for empirical observations [Source 3]. Stanford University research indicates that language models assign probabilities to sequences of words and generate text by sampling possible next words, trained on comprehensive datasets [Source 10]. The rapid evolution in this domain encompasses not only architectural improvements but also fundamental questions about efficiency, interpretability, and responsible deployment. Current models like T5 exploit transfer learning effectively through Text-to-Text Transfer Transformer approaches, demonstrating the versatility of unified frameworks [Source 9]. As this research area continues to expand, understanding the mathematical foundations, architectural innovations, and practical implications becomes crucial for advancing both theoretical knowledge and real-world applications.</p>

    <h1>Literature Review</h1>
    <p>The literature in this research area reveals a comprehensive evolution from theoretical foundations to practical implementations. Tai et al. (2025) provide crucial mathematical explanations of transformers for large language models and GPTs, establishing the theoretical framework that underpins current architectures [Source 1][Source 2]. This mathematical foundation is further extended by Chen (2025), who presents physical models realizing transformer architecture as open quantum systems in Fock space over the Hilbert space of tokens [Source 3][Source 4]. The efficiency challenges identified in the literature are addressed by Sun et al. (2025), whose survey on efficient architectures emphasizes that "speed always wins" in large language model design [Source 5]. This focus on efficiency is critical given the quadratic time and space complexities that pose significant computational resource challenges, as noted by Minaee et al. (2024), limiting input text length during training and inference [Source 8]. Huang et al. (2023) provide comprehensive coverage of transformer architecture advances in long-context large language models, highlighting developments in sparse mixture-of-experts layers that enable conditional computation and hybrid architectures that blend linear and standard attention mechanisms [Source 6][Source 7]. The survey work by Naveed et al. (2023) documents the scale of current models, noting systems with up to 137B parameters pre-trained on 1.56T words of public dialog data and web text [Source 9]. These literature sources collectively demonstrate the maturation of the field of Artificial Intelligence Trends from experimental approaches to mathematically grounded, scalable systems with clear theoretical underpinnings and practical applications.</p>

    <h2>Large Language Models and Generative AI Applications</h2>
    <p>Large language models represent the most visible advancement in this domain, with architectures scaling to unprecedented sizes and capabilities. Current models demonstrate remarkable scale, with systems reaching up to 137B parameters and pre-training on 1.56T words of public dialog data and web text [Source 9]. The mathematical foundation of these systems reveals sophisticated theoretical underpinnings, where transformer architectures function as discretizations of structured integro-differential equations [Source 1]. The self-attention mechanism, fundamental to transformer success, emerges naturally as a non-local integral operator, providing theoretical justification for empirical performance [Source 3]. Stanford University research demonstrates that language models assign probabilities to sequences of words and generate text by sampling possible next words, trained on comprehensive datasets that enable sophisticated text generation capabilities [Source 10]. The T5 model exemplifies effective transfer learning exploitation through Text-to-Text Transfer Transformer approaches, introducing unified frameworks that demonstrate versatility across multiple natural language processing tasks [Source 9]. Physical interpretations of these models have evolved to conceptualize large language models as open quantum systems in Fock space over the Hilbert space of tokens, providing novel theoretical perspectives on their operation [Source 4]. However, the quadratic time and space complexities of attention mechanisms pose significant computational resource challenges, limiting input text length during both training and inference phases [Source 8]. This constraint drives ongoing research into efficiency optimization and architectural innovations that maintain performance while reducing computational requirements.</p>

    <h2>Architectural Innovations and Efficiency Optimization</h2>
    <p>The field of Artificial Intelligence Trends research demonstrates critical focus on architectural efficiency, driven by the principle that "speed always wins" in large language model design [Source 5]. Recent surveys reveal comprehensive advances in sparse mixture-of-experts (MoE) layers that enable conditional computation, allowing models to activate only relevant parameters for specific inputs [Source 6]. Hybrid architectures represent another significant innovation, blending linear and standard attention mechanisms within or across layers to optimize computational efficiency while maintaining performance [Source 7]. These architectural improvements address the fundamental challenge of quadratic complexity in traditional attention mechanisms, which creates significant computational bottlenecks as sequence lengths increase [Source 8]. Long-context large language models require specialized architectural considerations, with research focusing on extending context windows while managing computational constraints [Source 6]. The emerging area of diffusion-based language generation architectures offers alternative approaches to traditional autoregressive generation, potentially providing new pathways for efficient text generation [Source 7]. Mathematical analysis reveals that layer normalization can be characterized within the integro-differential equation framework, providing theoretical grounding for architectural choices [Source 3]. The development of efficient architectures in this research area requires balancing multiple competing objectives: maintaining model capacity, reducing computational requirements, extending context length capabilities, and preserving generation quality. These architectural innovations represent crucial developments for the practical deployment of large language models in resource-constrained environments.</p>

    <h2>Mathematical Foundations and Theoretical Framework</h2>
    <p>The theoretical understanding of this domain has advanced significantly through mathematical formalization of transformer architectures. Research by Tai et al. (2025) establishes that transformers can be rigorously understood as discretizations of structured integro-differential equations, providing a mathematical foundation for their operation [Source 1][Source 2]. Within this formulation, the self-attention mechanism emerges naturally as a non-local integral operator, offering theoretical explanation for the mechanism's ability to capture long-range dependencies in sequences [Source 3]. Layer normalization, another critical component of transformer architecture, is characterized within this mathematical framework, providing theoretical justification for its effectiveness [Source 3]. The physical interpretation extends these mathematical foundations by constructing models that realize large language models as open quantum systems in Fock space over the Hilbert space of tokens [Source 4]. This quantum mechanical perspective offers novel insights into the operation of attention mechanisms and information processing in transformer architectures. The mathematical framework enables deeper understanding of why transformers excel at handling sequences with intricate relations, despite the computational challenges posed by their quadratic complexity [Source 8]. Stanford University research emphasizes the probabilistic foundations of language models, which assign probabilities to sequences of words through sophisticated mathematical operations on high-dimensional vector spaces [Source 10]. These theoretical advances in this research area provide essential groundwork for developing more efficient architectures, improving training methodologies, and extending model capabilities. The mathematical understanding also guides the development of hybrid architectures and specialized attention mechanisms that maintain theoretical soundness while achieving practical efficiency gains.</p>

    <h2>Scalability and Computational Challenges</h2>
    <p>This domain faces significant scalability challenges that drive architectural innovation and efficiency research. The fundamental issue stems from the quadratic time and space complexities inherent in attention mechanisms, which pose significant computational resource challenges and limit input text length during training and inference [Source 8]. Models with up to 137B parameters, trained on 1.56T words of data, exemplify the scale at which these computational challenges become critical [Source 9]. The principle that "speed always wins" in large language model design reflects the practical necessity of addressing these computational constraints [Source 5]. Research into efficient architectures focuses on developing solutions that maintain model capabilities while reducing computational requirements through innovations like sparse MoE layers and hybrid attention mechanisms [Source 6][Source 7]. Long-context processing represents a particular challenge, requiring specialized architectural approaches to handle extended sequences without overwhelming computational resources [Source 6]. The mathematical framework revealing transformers as discretizations of integro-differential equations provides theoretical guidance for developing more efficient implementations [Source 1]. Physical models conceptualizing transformers as quantum systems offer alternative perspectives on managing computational complexity [Source 4]. The Text-to-Text Transfer Transformer approach demonstrates how unified frameworks can improve efficiency by leveraging transfer learning across multiple tasks [Source 9]. Addressing scalability in this research area requires interdisciplinary approaches combining mathematical optimization, computer systems design, and algorithmic innovation. The development of diffusion-based language generation architectures represents one approach to circumventing traditional scaling limitations [Source 7]. Future progress in the field depends critically on resolving these scalability challenges while maintaining the sophisticated capabilities that make large language models effective.</p>

    <h1>Data & Analysis</h1>
    <p>Analysis of the available data reveals significant quantitative trends in this research area. Current large language models demonstrate unprecedented scale, with systems reaching up to 137B parameters and pre-training datasets comprising 1.56T words of public dialog data and web text [Source 9]. The mathematical analysis indicates that transformer architectures can be understood as discretizations of structured integro-differential equations, where self-attention mechanisms function as non-local integral operators [Source 1][Source 3]. Computational complexity analysis reveals quadratic time and space requirements that create significant resource challenges, particularly for processing long input sequences [Source 8]. The efficiency research documented in recent surveys shows that architectural innovations focus on conditional computation through sparse mixture-of-experts layers, which activate only relevant parameters for specific inputs [Source 6]. Performance data from Stanford University research demonstrates that language models achieve sophisticated text generation through probabilistic sampling across high-dimensional vector spaces [Source 10]. The T5 model architecture exemplifies transfer learning effectiveness, utilizing Text-to-Text Transfer Transformer approaches that demonstrate versatility across multiple natural language processing tasks [Source 9]. Physical modeling approaches reveal that transformer architectures can be realized as open quantum systems in Fock space, providing alternative theoretical frameworks for understanding their operation [Source 4]. Long-context processing capabilities show advances through hybrid architectures that blend linear and standard attention mechanisms within and across layers [Source 7]. The emerging area of diffusion-based language generation architectures offers quantifiable improvements in generation efficiency compared to traditional autoregressive approaches [Source 7]. These data points collectively indicate that the field of Artificial Intelligence Trends research is characterized by rapid scaling in model size and training data, accompanied by sophisticated mathematical understanding and innovative architectural approaches to address computational challenges.</p>

    <h1>Challenges</h1>
    <p>This research area faces several critical challenges that impede further advancement and practical deployment. The most significant challenge stems from the quadratic time and space complexities of attention mechanisms, which pose substantial computational resource challenges and limit input text length during both training and inference phases [Source 8]. Despite models reaching scales of up to 137B parameters trained on 1.56T words of data, the computational requirements create barriers to accessibility and deployment [Source 9]. Efficiency optimization remains a persistent challenge, with research emphasizing that "speed always wins" in large language model design, yet achieving this speed without compromising model capabilities proves difficult [Source 5]. Long-context processing presents particular difficulties, requiring specialized architectural innovations to handle extended sequences while managing computational constraints [Source 6]. The mathematical complexity underlying transformer architectures, while providing theoretical insights through integro-differential equation frameworks, also creates challenges for practitioners who must implement and optimize these sophisticated systems [Source 1][Source 3]. Architectural innovation challenges include developing effective sparse mixture-of-experts layers and hybrid attention mechanisms that maintain performance while reducing computational requirements [Source 6][Source 7]. The theoretical understanding of transformers as open quantum systems in Fock space, while mathematically elegant, presents implementation challenges for practical applications [Source 4]. Transfer learning effectiveness, demonstrated in Text-to-Text Transfer Transformer approaches, requires careful balance between generalization and specialization across different tasks [Source 9]. Resource accessibility challenges limit research participation to organizations with substantial computational infrastructure, potentially constraining innovation diversity. The emerging area of diffusion-based language generation architectures faces challenges in achieving competitive performance compared to established autoregressive approaches [Source 7]. Integration challenges arise when attempting to combine different architectural innovations while maintaining system stability and performance.</p>

    <h1>Future Outlook</h1>
    <p>The future of this domain appears promising, with several key directions emerging from current research trends. Mathematical foundations will likely continue evolving, building on the understanding of transformers as discretizations of structured integro-differential equations and the characterization of self-attention as non-local integral operators [Source 1][Source 3]. Physical modeling approaches may provide new theoretical insights, with quantum mechanical interpretations of transformer architectures as open systems in Fock space offering novel optimization pathways [Source 4]. Efficiency optimization will remain paramount, with the principle that "speed always wins" driving continued innovation in architectural design [Source 5]. Future developments in sparse mixture-of-experts layers and hybrid architectures that blend linear and standard attention mechanisms show promise for addressing scalability challenges [Source 6][Source 7]. Long-context processing capabilities will likely expand significantly, enabling applications requiring understanding of extended documents or complex reasoning chains [Source 6]. The diffusion-based language generation architectures represent an emerging area with potential to revolutionize text generation approaches beyond traditional autoregressive methods [Source 7]. Model scaling will continue, potentially surpassing the current 137B parameter models trained on 1.56T words, though efficiency considerations may drive architectural innovations rather than pure parameter scaling [Source 9]. Transfer learning approaches, exemplified by Text-to-Text Transfer Transformer methods, will likely become more sophisticated, enabling better generalization across diverse tasks [Source 9]. Theoretical understanding from Stanford University research suggests that probabilistic foundations will become more refined, improving text generation quality and controllability [Source 10]. The field of Artificial Intelligence Trends research is positioned to address current computational challenges through interdisciplinary collaboration combining mathematical optimization, computer systems design, and algorithmic innovation. Integration of multiple architectural innovations may yield hybrid systems that combine the benefits of different approaches while mitigating individual limitations. Future progress will likely focus on democratizing access to large language model capabilities through improved efficiency and reduced computational requirements.</p>

    <h1>Conclusion</h1>
    <p>This comprehensive analysis of the field of Artificial Intelligence Trends reveals a rapidly maturing research area characterized by significant theoretical advances, architectural innovations, and persistent computational challenges. The mathematical foundations established by recent research, particularly the understanding of transformers as discretizations of structured integro-differential equations with self-attention mechanisms functioning as non-local integral operators, provide crucial theoretical grounding for future developments [Source 1][Source 3]. The scale of current systems, with models reaching up to 137B parameters trained on 1.56T words of data, demonstrates the remarkable progress achieved in this domain [Source 9]. However, the quadratic time and space complexities that pose significant computational resource challenges highlight the critical importance of efficiency optimization [Source 8]. The principle that "speed always wins" in large language model design drives continued architectural innovation, including sparse mixture-of-experts layers and hybrid attention mechanisms [Source 5][Source 6][Source 7]. Physical interpretations of transformer architectures as open quantum systems offer novel theoretical perspectives that may guide future algorithmic developments [Source 4]. The success of transfer learning approaches, exemplified by Text-to-Text Transfer Transformer methods, indicates the potential for more sophisticated generalization capabilities [Source 9]. Long-context processing advances and emerging diffusion-based language generation architectures suggest that this research area will continue expanding its capabilities beyond current limitations [Source 6][Source 7]. The Stanford University research foundation emphasizing probabilistic text generation provides essential groundwork for improving controllability and quality [Source 10]. Looking forward, Artificial Intelligence Trends research faces the challenge of balancing continued capability expansion with accessibility and computational efficiency. The convergence of mathematical rigor, architectural innovation, and practical deployment considerations positions this domain as a cornerstone of future computational advancement. Success in addressing current challenges will determine the extent to which these technologies can be democratized and integrated into diverse applications, ultimately shaping the trajectory of computer science and human-machine interaction.</p>

    <div class="references">
        <h1>References</h1>
        <div class="ref-item">[1] X. Tai et al., "A Mathematical Explanation of Transformers for Large Language Models and GPTs," arXiv preprint arXiv:2510.03989, 2025. 
Link: https://arxiv.org/abs/2510.03989</div>
        <div class="ref-item">[2] X. Tai et al., "A Mathematical Explanation of Transformers for Large Language Models and GPTs," arXiv preprint arXiv:2510.03989, 2025. 
Link: https://arxiv.org/pdf/2510.03989</div>
        <div class="ref-item">[3] Z. Chen, "Physical models realizing the transformer architecture of large language models," arXiv preprint arXiv:2507.13354, 2025. 
Link: https://arxiv.org/pdf/2507.13354</div>
        <div class="ref-item">[4] W. Sun et al., "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models," arXiv preprint arXiv:2508.09834, 2025. 
Link: https://arxiv.org/abs/2508.09834</div>
        <div class="ref-item">[5] W. Sun et al., "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models," arXiv preprint arXiv:2508.09834, 2025. 
Link: https://arxiv.org/html/2508.09834v1</div>
        <div class="ref-item">[6] Y. Huang et al., "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey," arXiv preprint arXiv:2311.12351, 2023. 
Link: https://arxiv.org/abs/2311.12351</div>
        <div class="ref-item">[7] Y. Huang et al., "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey," arXiv preprint arXiv:2311.12351, 2023. 
Link: https://arxiv.org/html/2311.12351</div>
        <div class="ref-item">[8] S. Minaee et al., "Large Language Models: A Survey," arXiv preprint arXiv:2402.06196, 2024. 
Link: https://arxiv.org/pdf/2402.06196</div>
        <div class="ref-item">[9] H. Naveed et al., "A Comprehensive Overview of Large Language Models," arXiv preprint arXiv:2307.06435, 2023. 
Link: https://arxiv.org/pdf/2307.06435</div>
        <div class="ref-item">[10] Stanford Faculty, "Stanford University Research," Stanford University, 2024. 
Link: https://web.stanford.edu/class/cs124/lec/LLM2024.pdf</div>
        <div class="ref-item">[11] Stanford Faculty, "CME 295: Transformers & Large Language Models," Stanford University Course, 2024. 
Link: https://cme295.stanford.edu/slides/fall25-cme295-lecture9.pdf</div>
        <div class="ref-item">[12] Stanford Faculty, "CME 295: Transformers & Large Language Models," Stanford University Course, 2024. 
Link: https://cme295.stanford.edu/slides/fall25-cme295-lecture6.pdf</div>
        <div class="ref-item">[13] ArXiv Contributors, "ArXiv Preprint 2507.11545," arXiv, 2024. 
Link: https://arxiv.org/html/2507.11545v1</div>
        <div class="ref-item">[14] ArXiv Contributors, "ArXiv Preprint 2311.16851," arXiv, 2024. 
Link: https://arxiv.org/html/2311.16851</div>
        <div class="ref-item">[15] IEEE Authors, "IEEE Conference Paper," IEEE Xplore, 2024. 
Link: https://spectrum.ieee.org/edge-ai</div>
        <div class="ref-item">[16] IEEE Authors, "IEEE Conference Paper," IEEE Xplore, 2024. 
Link: https://ieeexplore.ieee.org/iel7/7693/8952731/08876870.pdf</div>
        <div class="ref-item">[17] IEEE Authors, "IEEE Document 10184528," IEEE Xplore, 2024. 
Link: https://ieeexplore.ieee.org/document/10184528/</div>
        <div class="ref-item">[18] IEEE Authors, "IEEE Document 10403985," IEEE Xplore, 2024. 
Link: https://ieeexplore.ieee.org/document/10403985/</div>
        <div class="ref-item">[19] IEEE Authors, "IEEE Document 10363514," IEEE Xplore, 2024. 
Link: https://ieeexplore.ieee.org/document/10363514/</div>
        <div class="ref-item">[20] ACM Authors, "ACM Paper DOI:10.1145/3486674," ACM Digital Library, 2024. 
Link: https://dl.acm.org/doi/10.1145/3486674</div>
        <div class="ref-item">[21] IEEE Authors, "IEEE Conference Paper," IEEE Xplore, 2024. 
Link: https://spectrum.ieee.org/edge-ai-multiverse-computing</div>
        <div class="ref-item">[22] ArXiv Contributors, "ArXiv Preprint 2501.02189," arXiv, 2024. 
Link: https://arxiv.org/html/2501.02189v6</div>
        <div class="ref-item">[23] ArXiv Contributors, "ArXiv Preprint 2403.17834," arXiv, 2024. 
Link: https://arxiv.org/pdf/2403.17834</div>
        <div class="ref-item">[24] ArXiv Contributors, "ArXiv Preprint 2401.17981," arXiv, 2024. 
Link: https://arxiv.org/html/2401.17981v1</div>
        <div class="ref-item">[25] ArXiv Contributors, "ArXiv Preprint 2509.23927," arXiv, 2024. 
Link: https://arxiv.org/html/2509.23927v3</div>
        <div class="ref-item">[26] ACM Authors, "ACM Paper DOI:10.1145/3696410.3714764," ACM Digital Library, 2024. 
Link: https://dl.acm.org/doi/10.1145/3696410.3714764</div>
        <div class="ref-item">[27] ACM Authors, "ACM Paper DOI:10.1145/3672758.3672824," ACM Digital Library, 2024. 
Link: https://dl.acm.org/doi/10.1145/3672758.3672824</div>
        <div class="ref-item">[28] ACM Authors, "ACM Research Paper," ACM Digital Library, 2025. 
Link: https://dl.acm.org/doi/10.1016/j.neucom.2025.130018</div>
        <div class="ref-item">[29] ACM Authors, "ACM Research Paper," ACM Digital Library, 2024. 
Link: https://dl.acm.org/doi/proceedings/10.1145/3746272</div>
        <div class="ref-item">[30] ACM Authors, "ACM Paper DOI:10.1145/3729343," ACM Digital Library, 2024. 
Link: https://dl.acm.org/doi/10.1145/3729343</div>
        <div class="ref-item">[31] IEEE Authors, "IEEE Document 10756172," IEEE Xplore, 2024. 
Link: https://ieeexplore.ieee.org/document/10756172/</div>

    </div>
</body>
</html>