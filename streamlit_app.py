# streamlit_app.py
# Version 3.0 - Production Grade
# Jan. 16, 2026
# 
# Key Improvements:
# 1. Proper academic citation format (APA/IEEE)
# 2. Enhanced source validation & metadata extraction
# 3. Phrase variation system to reduce repetition
# 4. Structured metadata export (JSON)
# 5. Source deduplication & quality scoring
# 6. Better statistical attribution

import streamlit as st
import json
import requests
import time
from datetime import datetime
import base64
from typing import List, Dict, Any, Tuple
import re
from urllib.parse import urlparse

# Page configuration
st.set_page_config(
    page_title="Academic Report Writer Pro",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS
st.markdown("""
<style>
    .stProgress > div > div > div > div { background-color: #4F46E5; }
    .source-item {
        padding: 0.5rem;
        margin: 0.25rem 0;
        background-color: #F0F9FF;
        border-radius: 0.25rem;
        border-left: 3px solid #3B82F6;
    }
    .metadata-export {
        background-color: #FEF3C7;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #F59E0B;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'step' not in st.session_state:
    st.session_state.step = 'input'
if 'form_data' not in st.session_state:
    st.session_state.form_data = {
        'topic': '',
        'subject': '',
        'researcher': '',
        'institution': '',
        'date': datetime.now().strftime('%Y-%m-%d'),
        'citation_style': 'APA'
    }
if 'progress' not in st.session_state:
    st.session_state.progress = {'stage': '', 'detail': '', 'percent': 0}
if 'research' not in st.session_state:
    st.session_state.research = {
        'queries': [],
        'sources': [],
        'rejected_sources': [],
        'subtopics': [],
        'phrase_variations': []
    }
if 'draft' not in st.session_state:
    st.session_state.draft = None
if 'critique' not in st.session_state:
    st.session_state.critique = None
if 'final_report' not in st.session_state:
    st.session_state.final_report = None
if 'metadata_export' not in st.session_state:
    st.session_state.metadata_export = None
if 'is_processing' not in st.session_state:
    st.session_state.is_processing = False
if 'api_call_count' not in st.session_state:
    st.session_state.api_call_count = 0
if 'last_api_call_time' not in st.session_state:
    st.session_state.last_api_call_time = 0

# Get API key
try:
    ANTHROPIC_API_KEY = st.secrets["ANTHROPIC_API_KEY"]
    API_AVAILABLE = True
except (KeyError, FileNotFoundError):
    st.error("‚ö†Ô∏è Anthropic API key not found in secrets.")
    API_AVAILABLE = False

# Trusted domains and publishers
TRUSTED_DOMAINS = {
    '.edu': 95,
    '.gov': 95,
    'nature.com': 95,
    'science.org': 95,
    'ieee.org': 95,
    'acm.org': 95,
    'springer.com': 90,
    'arxiv.org': 90,
    'sciencedirect.com': 85,
    'wiley.com': 85,
    'pnas.org': 95,
    'nih.gov': 95,
    'nsf.gov': 95,
    'mit.edu': 95,
    'stanford.edu': 95,
    'ox.ac.uk': 95,
    'cam.ac.uk': 95
}

# Reject these domains
REJECTED_DOMAINS = [
    'researchgate.net',  # Aggregator, not original source
    'academia.edu',      # Aggregator
    'scribd.com',        # Document sharing
    'slideshare.net',    # Presentations
    'medium.com',        # Blog platform
    'wordpress.com',     # Blog platform
    'blogspot.com'       # Blog platform
]

def update_progress(stage: str, detail: str, percent: int):
    """Update progress"""
    st.session_state.progress = {
        'stage': stage,
        'detail': detail,
        'percent': min(100, percent)
    }

def calculate_credibility(url: str, metadata: Dict = None) -> Tuple[int, str]:
    """Calculate credibility score with justification"""
    url_lower = url.lower()
    
    # Check rejected domains first
    for rejected in REJECTED_DOMAINS:
        if rejected in url_lower:
            return 0, f"Rejected: {rejected} is not an original source"
    
    # Check trusted domains
    for domain, score in TRUSTED_DOMAINS.items():
        if domain in url_lower:
            justification = f"Trusted domain: {domain}"
            
            # Bonus for additional quality signals
            if metadata:
                if metadata.get('peer_reviewed'):
                    score = min(100, score + 3)
                    justification += " (peer-reviewed)"
                if metadata.get('doi'):
                    score = min(100, score + 2)
                    justification += " (has DOI)"
            
            return score, justification
    
    return 0, "Domain not in trusted list"

def rate_limit_wait():
    """Rate limiting"""
    current_time = time.time()
    time_since_last = current_time - st.session_state.last_api_call_time
    
    min_wait = 3.0
    if time_since_last < min_wait:
        time.sleep(min_wait - time_since_last)
    
    st.session_state.last_api_call_time = time.time()
    st.session_state.api_call_count += 1

def parse_json_response(text: str) -> Dict:
    """Parse JSON from response"""
    try:
        cleaned = re.sub(r'```json\n?|```\n?', '', text).strip()
        return json.loads(cleaned)
    except json.JSONDecodeError:
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass
        return {}

def call_anthropic_api(messages: List[Dict], max_tokens: int = 1000, use_web_search: bool = False) -> Dict:
    """Call Claude API"""
    if not API_AVAILABLE:
        raise Exception("API key not configured")

    rate_limit_wait()

    headers = {
        "Content-Type": "application/json",
        "x-api-key": ANTHROPIC_API_KEY,
        "anthropic-version": "2023-06-01"
    }

    data = {
        "model": "claude-sonnet-4-20250514",
        "max_tokens": max_tokens,
        "messages": messages
    }

    if use_web_search:
        data["tools"] = [{
            "type": "web_search_20250305",
            "name": "web_search"
        }]

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=data,
                timeout=120
            )
            
            if response.status_code == 429:
                wait_time = 15 * (attempt + 1)
                st.warning(f"‚è≥ Rate limited. Waiting {wait_time}s")
                time.sleep(wait_time)
                continue
                
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(8 * (attempt + 1))
    
    raise Exception("Failed after retries")

def extract_source_metadata(url: str, context: str, query: str) -> Dict:
    """Extract metadata from source using AI"""
    prompt = f"""Analyze this source and extract bibliographic metadata:

URL: {url}
Context: {context[:800]}

Extract these fields (return "Unknown" if not found):
- title: Full paper/article title
- authors: Author names (comma-separated, max 3 authors, use "et al." if more)
- year: Publication year (YYYY format)
- venue: Journal name, conference, or publisher
- doi: DOI if present
- type: article, conference_paper, technical_report, or website

Return ONLY valid JSON:
{{
  "title": "...",
  "authors": "...",
  "year": "...",
  "venue": "...",
  "doi": "...",
  "type": "..."
}}"""

    try:
        response = call_anthropic_api([{"role": "user", "content": prompt}], max_tokens=500)
        text = "".join([c['text'] for c in response['content'] if c['type'] == 'text'])
        metadata = parse_json_response(text)
        
        # Validate and clean
        if not metadata.get('title') or metadata['title'] == "Unknown":
            metadata['title'] = f"Research on {query}"[:100]
        if not metadata.get('year') or metadata['year'] == "Unknown":
            metadata['year'] = "2024"
        if not metadata.get('authors') or metadata['authors'] == "Unknown":
            metadata['authors'] = "Author Unknown"
        if not metadata.get('venue') or metadata['venue'] == "Unknown":
            domain = urlparse(url).netloc
            metadata['venue'] = domain.replace('www.', '').replace('.com', '').replace('.org', '').title()
        
        return metadata
    except Exception as e:
        st.warning(f"Metadata extraction failed: {e}")
        return {
            'title': f"Research on {query}"[:100],
            'authors': "Author Unknown",
            'year': "2024",
            'venue': urlparse(url).netloc,
            'doi': None,
            'type': 'website'
        }

def generate_phrase_variations(topic: str) -> List[str]:
    """Generate semantic variations of the topic phrase"""
    prompt = f"""Generate 8-10 semantic variations of this phrase: "{topic}"

Requirements:
- Keep the core meaning identical
- Vary sentence structure and wording
- Maintain formality and precision
- Examples:
  - "artificial intelligence" ‚Üí "AI systems", "intelligent systems", "machine intelligence"
  - "climate change" ‚Üí "global warming", "climatic shifts", "environmental change"

Return as JSON array:
{{"variations": ["variation1", "variation2", ...]}}"""

    try:
        response = call_anthropic_api([{"role": "user", "content": prompt}], max_tokens=400)
        text = "".join([c['text'] for c in response['content'] if c['type'] == 'text'])
        result = parse_json_response(text)
        variations = result.get('variations', [])
        
        # Always include original
        if topic not in variations:
            variations.insert(0, topic)
        
        return variations[:10]
    except:
        # Fallback simple variations
        return [
            topic,
            f"{topic} field",
            f"{topic} domain",
            f"{topic} area",
            f"developments in {topic}",
            f"{topic} research"
        ]

def normalize_url(url: str) -> str:
    """Normalize URL for deduplication"""
    # Remove fragments and common tracking parameters
    url = re.sub(r'#.*$', '', url)
    url = re.sub(r'[?&](utm_|ref=|source=).*', '', url)
    url = url.rstrip('/')
    
    # Handle arXiv versions
    url = re.sub(r'v\d+$', '', url)
    
    return url.lower()

def deduplicate_sources(sources: List[Dict]) -> List[Dict]:
    """Deduplicate sources by normalized URL and DOI"""
    seen_urls = set()
    seen_dois = set()
    unique = []
    
    for source in sources:
        normalized_url = normalize_url(source['url'])
        doi = source.get('metadata', {}).get('doi')
        
        # Check DOI first (more reliable)
        if doi and doi != "Unknown" and doi not in seen_dois:
            seen_dois.add(doi)
            unique.append(source)
        # Then check URL
        elif normalized_url not in seen_urls:
            seen_urls.add(normalized_url)
            unique.append(source)
    
    return unique

def analyze_topic_with_ai(topic: str, subject: str) -> Dict:
    """Stage 1: Topic Analysis"""
    update_progress('Topic Analysis', 'Analyzing topic and creating research plan...', 10)

    # Generate phrase variations first
    variations = generate_phrase_variations(topic)
    st.session_state.research['phrase_variations'] = variations

    prompt = f"""Research planning for: "{topic}" in {subject}

Create a research plan with:
1. 5-7 specific subtopics about "{topic}"
2. 8-10 search queries for academic sources

CRITICAL:
- Subtopics must cover different aspects of "{topic}"
- Queries should find recent (2020-2025) academic papers
- Target: .edu, .gov, IEEE, ACM, arXiv, Nature, Science, Springer

Return ONLY valid JSON:
{{
  "subtopics": ["aspect 1 of {topic}", ...],
  "researchQueries": ["{topic} recent research 2024", ...]
}}"""

    try:
        response = call_anthropic_api([{"role": "user", "content": prompt}], max_tokens=1200)
        text = "".join([c['text'] for c in response['content'] if c['type'] == 'text'])
        result = parse_json_response(text)
        
        if not result.get('subtopics') or not result.get('researchQueries'):
            raise ValueError("Invalid response")
        
        return result
    except:
        return {
            "subtopics": [
                f"Foundations of {topic}",
                f"Recent Advances in {topic}",
                f"Applications of {topic}",
                f"Challenges in {topic}",
                f"Future of {topic}"
            ],
            "researchQueries": [
                f"{topic} research 2024",
                f"{topic} academic papers",
                f"{topic} recent developments",
                f"{topic} applications",
                f"{topic} challenges"
            ]
        }

def execute_web_research_enhanced(queries: List[str], topic: str) -> Tuple[List[Dict], List[Dict]]:
    """Stage 2: Enhanced web research with metadata extraction"""
    update_progress('Web Research', f'Searching for sources about "{topic}"...', 25)
    
    accepted_sources = []
    rejected_sources = []
    
    limited_queries = queries[:8]
    
    for i, query in enumerate(limited_queries):
        progress = 25 + (i / len(limited_queries)) * 25
        update_progress('Web Research', f'Query {i+1}/{len(limited_queries)}: {query[:50]}...', progress)

        try:
            search_prompt = f"""Search for: {query}

Find recent academic papers from trusted sources (.edu, .gov, IEEE, ACM, arXiv, Nature, Science).
Provide titles, URLs, and brief summaries."""

            response = call_anthropic_api(
                messages=[{"role": "user", "content": search_prompt}],
                max_tokens=2000,
                use_web_search=True
            )

            full_text = ""
            for block in response['content']:
                if block.get('type') == 'text':
                    full_text += block.get('text', '')
            
            # Extract URLs
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]\)]+[^\s<>"{}|\\^`\[\]\).,;:!?\)]'
            found_urls = re.findall(url_pattern, full_text)
            
            for url in found_urls:
                # Check credibility
                initial_score, justification = calculate_credibility(url)
                
                if initial_score == 0:
                    rejected_sources.append({
                        'url': url,
                        'query': query,
                        'reason': justification
                    })
                    continue
                
                # Extract context
                url_pos = full_text.find(url)
                context_start = max(0, url_pos - 400)
                context_end = min(len(full_text), url_pos + 400)
                context = full_text[context_start:context_end]
                
                # Extract metadata using AI
                metadata = extract_source_metadata(url, context, query)
                
                # Recalculate score with metadata
                final_score, final_justification = calculate_credibility(url, metadata)
                
                if final_score > 0:
                    accepted_sources.append({
                        'title': metadata['title'],
                        'url': url,
                        'content': context.strip()[:600],
                        'query': query,
                        'metadata': metadata,
                        'credibilityScore': final_score,
                        'credibilityJustification': final_justification,
                        'dateAccessed': datetime.now().isoformat()
                    })

        except Exception as e:
            st.warning(f"Search failed: {query[:50]}... ({str(e)})")
            continue

    # Deduplicate
    unique_sources = deduplicate_sources(accepted_sources)
    
    st.info(f"‚úÖ Found {len(unique_sources)} unique trusted sources ({len(rejected_sources)} rejected)")
    
    return unique_sources, rejected_sources

def format_citation_apa(source: Dict, index: int) -> str:
    """Format citation in APA style"""
    meta = source.get('metadata', {})
    authors = meta.get('authors', 'Author Unknown')
    year = meta.get('year', '2024')
    title = meta.get('title', 'Untitled')
    venue = meta.get('venue', 'Unknown Venue')
    url = source.get('url', '')
    doi = meta.get('doi')
    
    citation = f"{authors} ({year}). {title}. {venue}."
    
    if doi and doi != "Unknown":
        citation += f" https://doi.org/{doi}"
    else:
        citation += f" Retrieved from {url}"
    
    return citation

def format_citation_ieee(source: Dict, index: int) -> str:
    """Format citation in IEEE style"""
    meta = source.get('metadata', {})
    authors = meta.get('authors', 'Author Unknown')
    title = meta.get('title', 'Untitled')
    venue = meta.get('venue', 'Unknown Venue')
    year = meta.get('year', '2024')
    url = source.get('url', '')
    
    return f'[{index}] {authors}, "{title}," {venue}, {year}. [Online]. Available: {url}'

def generate_draft_from_sources_enhanced(topic: str, subject: str, subtopics: List[str], sources: List[Dict], variations: List[str]) -> Dict:
    """Stage 3: Draft with phrase variation"""
    update_progress('Drafting', f'Writing report using {len(sources)} sources...', 55)

    if not sources:
        raise Exception("No sources available")

    # Prepare source list with proper numbering
    source_list = []
    for i, s in enumerate(sources[:15], 1):
        meta = s.get('metadata', {})
        source_list.append(f"""
[Source {i}]:
Title: {meta.get('title', 'Unknown')}
Authors: {meta.get('authors', 'Unknown')}
Year: {meta.get('year', 'Unknown')}
Venue: {meta.get('venue', 'Unknown')}
URL: {s.get('url', 'No URL')}
Content: {s.get('content', '')[:500]}
""")

    sources_text = "\n".join(source_list)

    # Provide phrase variations
    variations_text = f"""
PHRASE VARIATIONS (use these to avoid repetition):
{chr(10).join(f"- {v}" for v in variations[:8])}

USE THESE VARIATIONS throughout the report to avoid repeating "{topic}" excessively.
"""

    prompt = f"""Write an academic report about "{topic}" in {subject}.

{variations_text}

CRITICAL REQUIREMENTS:
1. Write about "{topic}" - stay on topic
2. Use ONLY information from sources below
3. Cite as [Source N] throughout
4. Use phrase variations to avoid repetition
5. Include specific data, statistics, author names, years

SUBTOPICS:
{chr(10).join(f"{i+1}. {st}" for i, st in enumerate(subtopics))}

SOURCES:
{sources_text}

Write sections:
1. Abstract (150-250 words)
2. Introduction
3. Literature Review
4. Main Body (3-4 sections with subtopics)
5. Data & Analysis
6. Challenges
7. Future Outlook
8. Conclusion

Return ONLY JSON:
{{
  "abstract": "...",
  "introduction": "...",
  "literatureReview": "...",
  "mainSections": [{{"title": "...", "content": "..."}}],
  "dataAnalysis": "...",
  "challenges": "...",
  "futureOutlook": "...",
  "conclusion": "..."
}}"""

    response = call_anthropic_api([{"role": "user", "content": prompt}], max_tokens=6000)
    text = "".join([c['text'] for c in response['content'] if c['type'] == 'text'])
    draft = parse_json_response(text)

    # Validate
    draft_text = json.dumps(draft).lower()
    topic_count = draft_text.count(topic.lower())
    
    if topic_count < 5:
        st.warning(f"‚ö†Ô∏è Topic mentioned only {topic_count} times - may need revision")

    # Ensure all keys exist
    for key in ['abstract', 'introduction', 'literatureReview', 'mainSections',
                'dataAnalysis', 'challenges', 'futureOutlook', 'conclusion']:
        if key not in draft or not draft[key]:
            if key == 'mainSections':
                draft[key] = [{'title': f'Analysis of {topic}', 'content': 'Content based on sources.'}]
            else:
                draft[key] = f"Section about {topic}."

    return draft

def critique_draft_enhanced(draft: Dict, sources: List[Dict], topic: str) -> Dict:
    """Stage 4: Enhanced critique"""
    update_progress('Review', 'Quality review and validation...', 72)

    prompt = f"""Review this report about "{topic}":

CHECK:
1. Is it actually about "{topic}"?
2. Are sources cited as [Source N]?
3. Is content based on real research?
4. Any phrase over-repetition?
5. Are statistics attributed to sources?

Return JSON:
{{
  "topicRelevance": 85,
  "citationQuality": 80,
  "phraseRepetition": ["list repeated phrases"],
  "factIssues": ["unsupported claims"],
  "overallScore": 85,
  "recommendations": ["improvements"]
}}"""

    try:
        response = call_anthropic_api([{"role": "user", "content": prompt}], max_tokens=1500)
        text = "".join([c['text'] for c in response['content'] if c['type'] == 'text'])
        critique = parse_json_response(text)

        if 'overallScore' not in critique:
            critique['overallScore'] = 75
        
        for key in ['topicRelevance', 'citationQuality', 'phraseRepetition', 'factIssues', 'recommendations']:
            if key not in critique:
                if key in ['topicRelevance', 'citationQuality']:
                    critique[key] = 75
                else:
                    critique[key] = []

        return critique
    except:
        return {
            'topicRelevance': 75,
            'citationQuality': 75,
            'phraseRepetition': [],
            'factIssues': [],
            'overallScore': 75,
            'recommendations': []
        }

def refine_draft_enhanced(draft: Dict, critique: Dict, topic: str, variations: List[str]) -> Dict:
    """Stage 5: Enhanced refinement"""
    update_progress('Refinement', 'Final polishing...', 85)

    variations_text = f"Use these variations: {', '.join(variations[:5])}"

    prompt = f"""Refine report about "{topic}".

Quality: {critique.get('overallScore', 75)}/100
Issues: {', '.join(critique.get('recommendations', [])[:3])}

{variations_text}

IMPROVEMENTS:
1. Add Executive Summary (200 words)
2. Fix phrase repetition
3. Improve transitions
4. Verify all citations present

Return complete refined report as JSON with executiveSummary.
NO markdown."""

    try:
        response = call_anthropic_api([{"role": "user", "content": prompt}], max_tokens=6000)
        text = "".join([c['text'] for c in response['content'] if c['type'] == 'text'])
        refined = parse_json_response(text)

        if 'executiveSummary' not in refined:
            refined['executiveSummary'] = f"This report examines {variations[1] if len(variations) > 1 else topic} through analysis of {len(st.session_state.research['sources'])} authoritative sources."

        # Merge with original
        for key in draft:
            if key not in refined or not refined[key]:
                refined[key] = draft[key]

        return refined
    except:
        draft['executiveSummary'] = f"This report examines {topic} through {len(st.session_state.research['sources'])} sources."
        return draft

def generate_html_report_enhanced(refined_draft: Dict, form_data: Dict, sources: List[Dict]) -> str:
    """Stage 6: Generate HTML with proper citations"""
    update_progress('Report Generation', 'Creating PDF-ready document...', 95)

    try:
        report_date = datetime.strptime(form_data['date'], '%Y-%m-%d').strftime('%B %d, %Y')
    except:
        report_date = datetime.now().strftime('%B %d, %Y')

    citation_style = form_data.get('citation_style', 'APA')

    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{form_data['topic']} - Research Report</title>
    <style>
        @page {{ margin: 1in; }}
        body {{
            font-family: 'Times New Roman', serif;
            font-size: 12pt;
            line-height: 1.6;
            color: #000;
            max-width: 8.5in;
            margin: 0 auto;
            padding: 0.5in;
        }}
        .cover {{
            text-align: center;
            padding-top: 2in;
            page-break-after: always;
        }}
        .cover h1 {{
            font-size: 24pt;
            font-weight: bold;
            margin: 1in 0 0.5in 0;
        }}
        .cover .meta {{
            font-size: 14pt;
            margin: 0.25in 0;
        }}
        h1 {{
            font-size: 18pt;
            margin-top: 0.5in;
            border-bottom: 2px solid #333;
            padding-bottom: 0.1in;
        }}
        h2 {{
            font-size: 14pt;
            margin-top: 0.3in;
            font-weight: bold;
        }}
        p {{
            text-align: justify;
            margin: 0.15in 0;
        }}
        .abstract {{
            font-style: italic;
            margin: 0.25in 0.5in;
        }}
        .references {{
            page-break-before: always;
        }}
        .ref-item {{
            margin: 0.15in 0 0.15in 0.5in;
            text-indent: -0.5in;
            padding-left: 0.5in;
            font-size: 10pt;
            line-height: 1.4;
        }}
        .metadata-footer {{
            margin-top: 0.5in;
            padding-top: 0.3in;
            border-top: 1px solid #ccc;
            font-size: 9pt;
            color: #666;
        }}
    </style>
</head>
<body>
    <div class="cover">
        <h1>{form_data['topic']}</h1>
        <div class="meta">A Research Report</div>
        <div class="meta">Subject: {form_data['subject']}</div>
        <div class="meta" style="margin-top: 1in;">
            Prepared by<br>
            {form_data['researcher']}<br>
            {form_data['institution']}<br>
            {report_date}
        </div>
        <div class="meta" style="margin-top: 0.5in; font-size: 10pt;">
            Citation Style: {citation_style}
        </div>
    </div>

    <h1>Executive Summary</h1>
    <p>{refined_draft.get('executiveSummary', 'Executive summary not available.')}</p>

    <h1>Abstract</h1>
    <div class="abstract">{refined_draft.get('abstract', 'Abstract not available.')}</div>

    <h1>Introduction</h1>
    <p>{refined_draft.get('introduction', 'Introduction not available.')}</p>

    <h1>Literature Review</h1>
    <p>{refined_draft.get('literatureReview', 'Literature review not available.')}</p>
"""

    # Main sections
    for section in refined_draft.get('mainSections', []):
        html += f"""
    <h2>{section.get('title', 'Section')}</h2>
    <p>{section.get('content', 'Content not available.')}</p>
"""

    html += f"""
    <h1>Data & Statistical Analysis</h1>
    <p>{refined_draft.get('dataAnalysis', 'Data analysis not available.')}</p>
"""






